Time-optimal trajectory planning—generating motions that complete a task in the minimum possible time—is a fundamental requirement for agile robotic systems. From autonomous drone racing to emergency collision avoidance in self-driving cars, the ability to jointly optimize the control sequence and the total maneuver duration $T$ is critical for pushing physical limits. 
While Differential Dynamic Programming (DDP) and its variant, the iterative Linear Quadratic Regulator (iLQR), have become standard tools for high-dimensional trajectory optimization, they typically assume a fixed planning horizon. Extending these methods to time-optimal control introduces a discrete-continuous optimization challenge: the solver must determine the optimal integer horizon $T^*$ alongside the continuous control inputs.

Existing approaches to this problem generally fall into two categories: continuous-time relaxations and discrete search. 
Relaxation methods treat the final time as a continuous decision variable, typically by scaling the system dynamics. However, this often introduces non-convexity into the optimization landscape, leading to poor convergence.
Discrete search methods, on the other hand, treat $T$ as an integer parameter. A naive "brute-force" strategy involves evaluating every candidate horizon $T \in [T_{min}, T_{max}]$. 
\textbf{A fundamental bottleneck in this approach lies in the structure of the standard Riccati recursion.} In the LQR backward pass, the Value Function $V_k$ is computed recursively starting from a terminal cost anchored at the final time step $T$ (i.e., $P_T = Q_T$). Consequently, changing the horizon from $T$ to $T+1$ shifts the boundary condition, invalidating the entire sequence of previously computed Cost-to-Go matrices (P). This structural dependency prevents the reuse of historical computations across different horizons, forcing the solver to restart the backward pass from scratch for each candidate $T$, resulting in a prohibitive $\mathcal{O}(N^2)$ complexity.

To mitigate this computational burden, recent works such as the "One-Pass" method [1] have attempted to estimate costs for neighboring horizons by reusing the value function from a single nominal backward pass. While efficient for Linear Time-Invariant (LTI) systems where the dynamics do not shift with time, this approach fails for general trajectory optimization. In nonlinear systems, the local linearization ($A_k, B_k$) is time-varying; thus, reusing a fixed value function for different horizons introduces severe approximation errors, often leading to suboptimal horizon selection.

In this work, we propose a method that enables \textbf{exact computational reuse} for time-varying systems and extends it to the iLQR framework. 
Our approach builds on the observation that the Riccati difference equation can be viewed as a Linear Fractional Transformation (LFT). By reformulating the backward pass in information form, we construct a "propagator" that allows us to compose the inverse value functions incrementally. This structure decouples the backward recursion from the specific terminal time $T$, allowing us to query the optimal cost for \textit{any} candidate horizon using the same pre-computed propagator sequence. 
Furthermore, to apply this logic to iLQR, we introduce an augmented state space formulation that absorbs the time-varying affine linearization terms. This unifies the treatment of linear and nonlinear problems, allowing the propagator to compute the \textit{exact} LQR cost for all horizons in a single $\mathcal{O}(N)$ pass.

The main contributions of this paper are:
\begin{enumerate}
    \item \textbf{Propagator-based Horizon Selection:} We develop an LFT-based solver that enables the reuse of backward pass computations, reducing the complexity of horizon selection from $\mathcal{O}(N^2n^3)$ to $\mathcal{O}(Nn^3)$.
    \item \textbf{Augmented State Formulation:} We propose a state augmentation technique that embeds affine linearization terms into a homogeneous coordinate system, extending the efficient propagator method to general nonlinear iLQR problems.
    \item \textbf{Performance and Robustness:} We validate our algorithm on four benchmark systems, including a 12-DOF Quadrotor. Experimental results show that our method achieves speedups of up to $43\times$ compared to brute-force search while guaranteeing global optimality with respect to the linearized model. 
\end{enumerate}