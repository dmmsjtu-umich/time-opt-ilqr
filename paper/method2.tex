
\subsection{Augmented State Space and Dynamics}

\subsection{\abbrAlgg Algorithm}

\subsection{Method 2: Augmented State Space for Time-Optimal iLQR}

Standard iterative Linear Quadratic Regulator (iLQR) is a trajectory optimization technique that solves nonlinear optimal control problems by iteratively approximating them as Linear Time-Varying (LTV) LQR sub-problems. In its classical form, iLQR operates on a fixed time horizon $N$. To extend this to time-optimal control where the horizon $T$ is a decision variable, we propose a strategy that embeds the efficient horizon selection developed in the previous section into the iLQR loop.

\paragraph*{Core Idea}
The central concept of our approach is to transform the local approximation generated by iLQR into a format compatible with the \textbf{Propagator-based Solver} (described in Section~\ref{sec:method1}). The process proceeds as follows:
\begin{enumerate}
    \item \textbf{Approximation:} At each iteration, we linearize the nonlinear dynamics and quadratize the cost function around the current nominal trajectory. This yields a local LTV optimal control problem with affine terms (due to linearization offsets).
    \item \textbf{Transformation:} We employ an \textit{Augmented State Space} formulation (introducing a homogeneous coordinate $z_k = [\delta x_k; 1]$) to absorb the linear and constant terms. This converts the affine LTV problem into a standard quadratic LQR form.
    \item \textbf{Horizon Selection:} With the problem now in standard form, we directly apply the Propagator method from Section~\ref{sec:method1} to efficiently evaluate the cost for all candidate horizons $t \in [T_{\min}, T_{\max}]$ and select the optimal $T^*$.
    \item \textbf{Update:} We perform the trajectory update using the feedback gains derived for this optimal horizon $T^*$.
\end{enumerate}

This formulation allows us to leverage the $O(N)$ complexity of the propagator method within the general nonlinear control framework.

\subsubsection{Linearization and Quadratization}

Given a nominal trajectory $(\bar{x}_k, \bar{u}_k)$, we define deviations $\delta x_k := x_k - \bar{x}_k$ and $\delta u_k := u_k - \bar{u}_k$. The linearized dynamics become:
\[
\delta x_{k+1} = A_k \delta x_k + B_k \delta u_k + a_k,
\]
where $A_k = \nabla_x f(\bar{x}_k, \bar{u}_k)$, $B_k = \nabla_u f(\bar{x}_k, \bar{u}_k)$, and $a_k = f(\bar{x}_k, \bar{u}_k) - \bar{x}_{k+1}$ captures the affine term.

The stage cost is expanded to second order:
\[
\begin{aligned}
\ell(&\bar{x}_k + \delta x_k, \bar{u}_k + \delta u_k) \approx \\
&\ell(\bar{x}_k, \bar{u}_k) + w + \ell_{x,k}^\top \delta x_k + \ell_{u,k}^\top \delta u_k \\
&+ \tfrac{1}{2} \delta x_k^\top \ell_{xx,k} \delta x_k + \delta x_k^\top \ell_{xu,k} \delta u_k \\
&+ \tfrac{1}{2} \delta u_k^\top \ell_{uu,k} \delta u_k,
\end{aligned}
\]
where subscripts denote partial derivatives evaluated at $(\bar{x}_k, \bar{u}_k)$, and the time penalty $w$ is included in the constant term.

\subsubsection{Pure Quadratization via Completing the Square}

The cross-term $\delta x_k^\top \ell_{xu,k} \delta u_k$ prevents direct application of standard LQR methods. To eliminate it and achieve pure quadratic form, we complete the square for control terms. All terms containing $\delta u_k$ are:
\[
\tfrac{1}{2} \delta u_k^\top \ell_{uu,k} \delta u_k + (\ell_{xu,k}^\top \delta x_k + \ell_{u,k})^\top \delta u_k.
\]

Define the shifted control variable:
\begin{equation}\label{eq:shifted_control}
v_k := \delta u_k + \ell_{uu,k}^{-1}(\ell_{ux,k} \delta x_k + \ell_{u,k}).
\end{equation}

After completing the square, the stage cost becomes:
\[
\begin{aligned}
\ell_k \approx& \; \tfrac{1}{2} \delta x_k^\top \underbrace{(\ell_{xx,k} - \ell_{xu,k} \ell_{uu,k}^{-1} \ell_{ux,k})}_{\text{modified } Q} \delta x_k \\
&+ \underbrace{(\ell_{x,k} - \ell_{xu,k} \ell_{uu,k}^{-1} \ell_{u,k})^\top}_{\text{modified linear term}} \delta x_k \\
&+ \tfrac{1}{2} v_k^\top \ell_{uu,k} v_k \\
&+ \underbrace{\left(\ell(\bar{x}_k, \bar{u}_k) + w - \tfrac{1}{2} \ell_{u,k}^\top \ell_{uu,k}^{-1} \ell_{u,k}\right)}_{\text{constant term with time penalty}}.
\end{aligned}
\]

\subsubsection{State Augmentation for Affine Terms}

To handle linear and constant terms systematically within the LQR framework, we augment the state with a unit element:
\[
z_k = \begin{bmatrix} \delta x_k \\ 1 \end{bmatrix}.
\]

This augmentation allows us to express all terms in pure quadratic form. Define the shorthand notation:
\[
\tilde{Q}_k = \ell_{xx,k} - \ell_{xu,k} \ell_{uu,k}^{-1} \ell_{ux,k}, \quad
\tilde{q}_k = \ell_{x,k} - \ell_{xu,k} \ell_{uu,k}^{-1} \ell_{u,k}.
\]
Then the augmented cost matrix becomes:
\begin{equation}\label{eq:aug_cost_matrices}
Q_k^{\text{aug}} = \begin{bmatrix}
\tilde{Q}_k & \tilde{q}_k \\
\tilde{q}_k^\top & 2\left(\ell(\bar{x}_k, \bar{u}_k) + w - \tfrac{1}{2} \ell_{u,k}^\top \ell_{uu,k}^{-1} \ell_{u,k}\right)
\end{bmatrix}.
\end{equation}
\[
R_k = \ell_{uu,k}.
\]

The stage cost in augmented form achieves the desired pure quadratic structure:
\begin{equation}\label{eq:aug_stage_cost}
\boxed{\ell_k \approx \tfrac{1}{2} z_k^\top Q_k^{\text{aug}} z_k + \tfrac{1}{2} v_k^\top R_k v_k}
\end{equation}

Similarly, the terminal cost matrix:
\[
Q_T^{\text{aug}} = \begin{bmatrix}
\phi_{xx,T} & \phi_{x,T} \\
\phi_{x,T}^\top & 2\phi(\bar{x}_T)
\end{bmatrix}.
\]

\subsubsection{Augmented Dynamics}

The linearized dynamics must also be transformed to work with the shifted control $v_k$:
\[
\delta x_{k+1} = \underbrace{(A_k - B_k \ell_{uu,k}^{-1} \ell_{ux,k})}_{\hat{A}_k} \delta x_k + B_k v_k \underbrace{- B_k \ell_{uu,k}^{-1} \ell_{u,k}}_{\tilde{a}_k}.
\]

The augmented system matrices that preserve the affine structure:
\begin{equation}\label{eq:aug_dynamics}
A_k^{\text{aug}} = \begin{bmatrix}
A_k - B_k \ell_{uu,k}^{-1} \ell_{ux,k} & -B_k \ell_{uu,k}^{-1} \ell_{u,k} \\
0 & 1
\end{bmatrix}, \quad
B_k^{\text{aug}} = \begin{bmatrix}
B_k \\ 0
\end{bmatrix}.
\end{equation}

\subsubsection{Integration with Propagator Method}

The key innovation is that the augmented system $(A_k^{\text{aug}}, B_k^{\text{aug}}, Q_k^{\text{aug}}, R_k)$ is now in standard LQR form, enabling direct application of the propagator method from Method 1. We compute:
\[
\begin{aligned}
E_k &= (Q_k^{\text{aug}})^{-1}, \\
F_k &= E_k (A_k^{\text{aug}})^\top, \\
G_k &= A_k^{\text{aug}} E_k (A_k^{\text{aug}})^\top + B_k^{\text{aug}} R_k^{-1} (B_k^{\text{aug}})^\top.
\end{aligned}
\]

Using the prefix composition formula (Eq.~\ref{eq:prefix_recursion}), we efficiently obtain propagator parameters $(\bar{E}_{t-1}, \bar{F}_{t-1}, \bar{G}_{t-1})$ for all candidate horizons. The cost for horizon $t$ is:
\begin{equation}\label{eq:horizon_cost}
J_t = \tfrac{1}{2} z_0^\top (\tilde{P}_0^{(t)})^{-1} z_0,
\end{equation}
where $\tilde{P}_0^{(t)} = \bar{E}_{t-1} - \bar{F}_{t-1}(\tilde{P}_T + \bar{G}_{t-1})^{-1} \bar{F}_{t-1}^\top$ and $\tilde{P}_T = (Q_T^{\text{aug}} + \varepsilon I)^{-1}$ with small regularization $\varepsilon > 0$.

The optimal horizon is selected as:
\[
T^* = \arg\min_{t \in [T_{\min}, T_{\max}]} J_t.
\]

This selection step, which would normally require $\mathcal{O}(N^2n^3)$ operations if solved independently for each horizon, now requires only $\mathcal{O}(Nn^3)$ using the propagator method.

\subsubsection{Truncated Backward Pass and Control Recovery}

After selecting $T^*$ using the propagator method, we perform a standard iLQR backward pass, but crucially only on the truncated interval $[0, T^*-1]$. This truncation provides significant computational savings compared to always computing over the full horizon $N$.

The backward pass computes value functions and feedback gains. Starting from terminal conditions at $T^*$:
\[
V_{xx,T^*} = \phi_{xx,T^*}, \quad V_{x,T^*} = \phi_{x,T^*}, \quad V_{0,T^*} = \phi(\bar{x}_{T^*}),
\]
we recursively compute Q-function derivatives and feedback gains as detailed in Algorithm~\ref{alg:time-optimal-ilqr}.

The feedback gain from the augmented system has the form:
\[
v_k = -K_k^{(v)} z_k, \quad \text{where} \quad K_k^{(v)} = [K_{x,k}^{(v)} \; k_{\rho,k}^{(v)}].
\]

To recover the original control deviation, we invert the transformation in Eq.~\eqref{eq:shifted_control}:
\begin{equation}\label{eq:control_recovery}
\delta u_k^* = -(K_{x,k}^{(v)} + \ell_{uu,k}^{-1} \ell_{ux,k}) \delta x_k - (k_{\rho,k}^{(v)} + \ell_{uu,k}^{-1} \ell_{u,k}).
\end{equation}
\begin{algorithm}[t]
\SetAlgoLined
\DontPrintSemicolon
\caption{Time-Optimal iLQR with Propagator Selection}
\label{alg:time-optimal-ilqr}

\KwIn{Dynamics $f$, costs $\ell, \phi$, initial state $x_0$, initial controls $U$ (or zeros), horizon bounds $[T_{\min}, T_{\max}]$, time penalty $w$}
\KwOut{Optimal trajectory and controls}

\Repeat{convergence}{
    \tcp{Step 1: Linearization and augmentation}
    Rollout $X$ using $f$ and $U$\;
    \For{$k = 0$ \KwTo $N-1$}{
        Compute $A_k, B_k$ at $(\bar{x}_k, \bar{u}_k)$\\
        Compute derivatives: $\ell_{x,k}, \ell_{u,k}, \ell_{xx,k}, \ell_{ux,k}, \ell_{uu,k}$\\
        Build $A_k^{\text{aug}}, B_k^{\text{aug}}, Q_k^{\text{aug}}, R_k$ via Eqs.~\eqref{eq:aug_cost_matrices}-\eqref{eq:aug_dynamics}\;
    }
    Build $Q_T^{\text{aug}}$ from $\phi$ at $\bar{x}_T$\;
    
    \BlankLine
    \tcp{Step 2: Propagator-based horizon selection}
    $J \gets$ PropagatorCost$(A^{\text{aug}}, B^{\text{aug}}, Q^{\text{aug}}, R, z_0, Q_T^{\text{aug}}, N)$\;
    $T^* \gets \arg\min_{t \in [T_{\min}, T_{\max}]} J[t]$\;
    
    \BlankLine
    \tcp{Step 3: Backward pass on $[0, T^*-1]$}
    Initialize: 
    
    $V_{xx}[T^*] \gets \phi_{xx}$, $V_x[T^*] \gets \phi_x$, $V_0[T^*] \gets \phi(\bar{x}_{T^*})$\\
    \For{$k = T^*-1$ \KwDownTo $0$}{
        \tcp{Compute Q-function derivatives}
        $Q_x \gets \ell_{x,k} + A_k^\top V_{x,k+1}$\\
        $Q_u \gets \ell_{u,k} + B_k^\top V_{x,k+1}$\\
        $Q_{xx} \gets \ell_{xx,k} + A_k^\top V_{xx,k+1} A_k$\\
        $Q_{ux} \gets \ell_{ux,k} + B_k^\top V_{xx,k+1} A_k$\\
        $Q_{uu} \gets \ell_{uu,k} + B_k^\top V_{xx,k+1} B_k$ (+ LM reg.)\;
        
        \tcp{Compute gains}
        $\kappa_k \gets -Q_{uu}^{-1} Q_u$, $K_k \gets -Q_{uu}^{-1} Q_{ux}$\\
        
        \tcp{Update value function}
        $V_{xx,k} \gets Q_{xx} - Q_{ux}^\top Q_{uu}^{-1} Q_{ux}$\\
        $V_{x,k} \gets Q_x - Q_{ux}^\top Q_{uu}^{-1} Q_u$\\
        $V_{0,k} \gets V_{0,k+1} + (\ell(\bar{x}_k, \bar{u}_k) + w) - \tfrac{1}{2} Q_u^\top Q_{uu}^{-1} Q_u$\\
    }
    
    \BlankLine
    \tcp{Step 4: Forward rollout with line search}
    \For{$\alpha \in \{1, 0.5, 0.25, 0.1\}$}{
        $x_{\text{new}}[0] \gets x_0$\\
        \For{$k = 0$ \KwTo $T^*-1$}{
            $\delta x \gets x_{\text{new}}[k] - \bar{x}_k$\\
            $\delta u \gets K_k \cdot \delta x + \alpha \cdot \kappa_k$\\
            $u_{\text{new}}[k] \gets \bar{u}_k + \delta u$\\
            $x_{\text{new}}[k+1] \gets f(x_{\text{new}}[k], u_{\text{new}}[k])$\;
        }
        \If{TrueCost$(x_{\text{new}}, u_{\text{new}}, T^*) <$ TrueCost$(\bar{x}, \bar{u}, T^*)$}{
            Accept trajectory; \textbf{break}\;
        }
    }
    \If{not accepted}{Increase LM regularization}
    \Else{Update nominal $(X, U)$}
}
\end{algorithm}
\subsubsection{Complete Algorithm}




Algorithm~\ref{alg:time-optimal-ilqr} presents the complete time-optimal iLQR procedure with propagator-based horizon selection. The algorithm alternates between:
1. Linearization and augmentation around the current nominal trajectory
2. Efficient horizon selection using the propagator method
3. Truncated backward pass on the optimal horizon
4. Forward rollout with line search for robust convergence

The integration of the augmented state space formulation with the propagator method is the key to achieving computational efficiency while handling the full nonlinear, time-optimal control problem.



\subsubsection{Complexity Analysis and Advantages}

The augmented propagator approach achieves significant computational advantages:

\begin{itemize}
    \item \textbf{Horizon selection}: $\mathcal{O}(Nn^3)$ for evaluating all candidate horizons using the propagator method, compared to $\mathcal{O}(N^2n^3)$ for brute-force evaluation
    \item \textbf{Per iteration}: Only one backward pass on the truncated interval $[0, T^*-1]$ instead of $N$ separate iLQR runs for different horizons
\end{itemize}

\textbf{Key innovation:} The augmented state space formulation transforms the nonlinear time-optimal control problem into a form amenable to efficient LFT-based computation. This unifies the treatment of linear and nonlinear casesâ€”the nonlinear problem, after linearization and augmentation, can leverage the same efficient propagator machinery developed for the LTV case in Method 1. The result is a practical algorithm that achieves near-optimal time-optimal control for complex nonlinear systems with computational cost comparable to standard fixed-horizon iLQR.